\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage[bookmarks=true]{hyperref}

\pdfinfo{
   /Author (Homer Simpson)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}

\begin{document}

% paper title
\title{Inverse Reinforcement Learning from Failure}

% You will get a Paper-ID when submitting a pdf file to the conference system
\author{Kyriacos Shiarlis, Joao Messias, Maarten Van Someren, Shimon Whiteson}

%\author{\authorblockN{Michael Shell}
%\authorblockA{School of Electrical and\\Computer Engineering\\
%Georgia Institute of Technology\\
%Atlanta, Georgia 30332--0250\\
%Email: mshell@ece.gatech.edu}
%\and
%\authorblockN{Homer Simpson}
%\authorblockA{Twentieth Century Fox\\
%Springfield, USA\\
%Email: homer@thesimpsons.com}
%\and
%\authorblockN{James Kirk\\ and Montgomery Scott}
%\authorblockA{Starfleet Academy\\
%San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212\\
%Fax: (888) 555--1212}}


% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership


% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\authorblockN{Michael Shell\authorrefmark{1},
%Homer Simpson\authorrefmark{2},
%James Kirk\authorrefmark{3}, 
%Montgomery Scott\authorrefmark{3} and
%Eldon Tyrell\authorrefmark{4}}
%\authorblockA{\authorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: mshell@ece.gatech.edu}
%\authorblockA{\authorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\authorblockA{\authorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\authorblockA{\authorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}


\maketitle

\begin{abstract}
In this paper we approach the problem of Inverse Reinforcement Learning from a rather different perspective. Instead of trying to only mimic an expert as in 
traditional IRL, we present a method that can utilise information from failed, bad or simply opposite demonstrations of a task that we are trying to learn. 
We achieve this by modifying the state-of-the-art method of Maximum Causal Entropy Inverse Reinforcement Learning, and show in experimental results, that our
method can converge faster and learn better than its original counterpart, at no extra computational cost. 
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction and Motivation}
Inverse reinforcement Learning (IRL) has been subject to a significant amount of research since its introduction to the computer science research community by Ng and Russel \cite{ng2000algorithms}. The problem involves an agent or \emph{apprentice} acting in an environment modelled by a Markov Decision Process (MDP) for which the reward function is not available but samples from the policy of an \emph{expert} performing the decision-making, are given instead. An IRL algorithm tries to find a reward function that allows the apprentice to exhibit similar behaviour as the expert and generalises well to situations for which data is not available. The IRL formulation is particularly appealing in Markovian Decision Tasks for which the actual reward function is very difficult to define explicitly, but examples of correct behaviour can be generated instead. For this exact reason the main applications of the method have been in simulated car driving[] and socially appropriate navigation\cite{henry2010learning}\cite{vasquez2014inverse}. To achieve this researchers have  adapted concepts from the wider Machine Learning community, like maximum entropy\cite{ziebart2008maximum} and bayesian formulations \cite{ramachandran2007bayesian}, structured classification \cite{ratliff2006maximum}, boosting \cite{ratliff2007boosting} and gaussian processes\cite{levine2011nonlinear} with varying degrees of success.\\

Despite its appealing applications and encouraging research the IRL framework still suffers from a number of important theoretical and practical drawbacks. Firstly, because of their iterative nature most known methods require solving an MDP under the current reward before an update. For large state spaces, Markov decision processes becomes prohibitive to evaluate once, let alone a number of times. Secondly, most algorithms are non-convex in nature especially if the dynamics of the system are non-deterministic and ill posed how do I make the differentiation? More practical issues include generalisation, if the observed expert's behaviour is too limited, it is very hard to generalise to new parts of the state space, especially if this is too large. In addition, since the expert can be expected to avoid punishing states, information about the reward in these states reaches us only implicitly, making generalisation of the reward function an even harder task. \\

We present the first step towards allowing IRL algorithms to leverage a wider variety of data to learn faster and generalise better. Specifically we consider the case where appart from
an Expert dataset we have access to a dataset of behaviour that should be avoided at all costs. Our algorithm tries to approach the expert behaviour at while avoiding the failed trajectories.

This altered view of IRL emerges from the basic principle that governs most IRL algorithms. Because the observed data is assumed to be coming from an Expert, IRL tries to make the value of the observed trajectories optimal by tweeking the reward function in an intelligent way, most commonly by gradient (or subgradient) descent. We can imagine however that we observe a certain trajectory in the state space and we have access to an annotator that can tell us to what \emph{degree} the observe behaviour is optimal. If for example we observe the worst possible behaviour, then we can possibly demand from our algorithm to \emph{minimise} value for those trajectories when generated by the apprentice. Importanly, this view preserves the main motivation for IRL which is that the reward function for each state-action pair need not be hardcoded, while \emph{extending} the scope from which demonstrations can be considered useful to learning. In addition, by exploiting the information contained within, for example, failed trajectories, we gain access to more explicit information about the nature of the reward at certain areas of the state-space that the expert could only implicitly provide. This provides additional constraints to combat the aforementioned illposition of IRL and allow's us to build reward functions that are more likely to generalise to unseen situations. Finally such a framework would allow the designer to inject prior knowledge into the learning process in a principled manner, without the need for complex prior distributions and extra computational cost. If we are previously aware of parts of the state-space that should have a very negative reward, we can (assuming the dynamics are known) simulate data of the agent acting in an environment where large rewards are given for reaching unwanted states. Feeding our generated data along with that of the expert into our extended IRL algorithm would allow the apprentice to imitate the expert while learning a reward function that takes into account our domain knowledge.



\section{Method Outline}
	\begin{itemize}	
		\item Our method takes the original Maximum Causal Entropy for Inverse Optimal Control posed by Ziebart[] and rephrases it as follows. Put equation mention constraints of largest importance. also the alphas. 
		\item Show how you are adding more constraints to the problem.
		\item Give an outline of how you solve. 
	\end{itemize}

\section{Experiments}
Explain moving obstacle Gridworld. Explain your metrics. Put figures and exlain what they demonstrate.

\section{Conclusions and further work}
Proof of concept that failed trajectories contain valuable information for learning in many fronts. 
Further work includes:
\begin{itemize}
	\item Tackling problems of having very similar data from expert and non expert.
	\item Applying concept to other methods that might require different treatment.
\end{itemize}


\section{RSS citations}

Please make sure to include \verb!natbib.sty! and to use the
\verb!plainnat.bst! bibliography style. \verb!natbib! provides additional
citation commands, most usefully \verb!\citet!. For example, rather than the
awkward construction 

{\small
\begin{verbatim}
\cite{kalman1960new} demonstrated...
\end{verbatim}
}

\noindent
rendered as ``\cite{kalman1960new} demonstrated...,''
or the
inconvenient 

{\small
\begin{verbatim}
Kalman \cite{kalman1960new} 
demonstrated...
\end{verbatim}
}

\noindent
rendered as 
``Kalman \cite{kalman1960new} demonstrated...'', 
one can
write 

{\small
\begin{verbatim}
\citet{kalman1960new} demonstrated... 
\end{verbatim}
}
\noindent
which renders as ``\citet{kalman1960new} demonstrated...'' and is 
both easy to write and much easier to read.
  
\subsection{RSS Hyperlinks}

This year, we would like to use the ability of PDF viewers to interpret
hyperlinks, specifically to allow each reference in the bibliography to be a
link to an online version of the reference. 
As an example, if you were to cite ``Passive Dynamic Walking''
\cite{McGeer01041990}, the entry in the bibtex would read:

{\small
\begin{verbatim}
@article{McGeer01041990,
  author = {McGeer, Tad}, 
  title = {\href{http://ijr.sagepub.com/content/9/2/62.abstract}{Passive Dynamic Walking}}, 
  volume = {9}, 
  number = {2}, 
  pages = {62-82}, 
  year = {1990}, 
  doi = {10.1177/027836499000900206}, 
  URL = {http://ijr.sagepub.com/content/9/2/62.abstract}, 
  eprint = {http://ijr.sagepub.com/content/9/2/62.full.pdf+html}, 
  journal = {The International Journal of Robotics Research}
}
\end{verbatim}
}
\noindent
and the entry in the compiled PDF would look like:

\def\tmplabel#1{[#1]}

\begin{enumerate}
\item[\tmplabel{1}] Tad McGeer. \href{http://ijr.sagepub.com/content/9/2/62.abstract}{Passive Dynamic
Walking}. {\em The International Journal of Robotics Research}, 9(2):62--82,
1990.
\end{enumerate}
%
where the title of the article is a link that takes you to the article on IJRR's website. 


Linking cited articles will not always be possible, especially for
older articles. There are also often several versions of papers
online: authors are free to decide what to use as the link destination
yet we strongly encourage to link to archival or publisher sites
(such as IEEE Xplore or Sage Journals).  We encourage all authors to use this feature to
the extent possible.
\section*{Acknowledgments}

%% Use plainnat to work nicely with natbib. 

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}


